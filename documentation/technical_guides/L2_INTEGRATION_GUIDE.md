# Level 2 Data Integration Guide

## ðŸ”¬ **CRITICAL UPDATE: TA Indicators Eliminated**

**SHAP analysis revealed that ALL traditional TA indicators have ZERO predictive importance when HHT and L2 features are available:**

### Eliminated Features (0.000000 SHAP importance):
- RSI, MACD (all variants), Bollinger Bands (all variants)
- ATR, KAMA, SuperTrend, VWAP

### Dominant Features:
1. **HHT Features**: 0.071870 average importance (top predictor)
2. **L2 Features**: 0.034098 average importance (microstructure signals)
3. **OHLCV Features**: 0.046007 average importance (price/volume)

### Configuration Impact:
```yaml
# OLD: 45 features (including 15 useless TA indicators)
ta_features: ['rsi', 'macd', 'bbands', 'atr', 'kama', 'supertrend', 'vwap']

# NEW: 30 optimized features (TA completely removed)
ta_features: []  # DISABLED - Zero predictive value
```

**Result**: Cleaner model, faster training, same/better performance with 33% fewer features.

---

## Overview
This guide explains how to integrate Level 2 (order book) data into the trading bot system for model training, backtesting, and live trading.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L2 Data Collector  â”‚â”€â”€â”€â”€â–¶â”‚  L2 Data Storage â”‚â”€â”€â”€â”€â–¶â”‚ Feature Engineerâ”‚
â”‚  (WebSocket)        â”‚     â”‚  (Compressed)    â”‚     â”‚ (L2 Features)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Data Alignment  â”‚
                            â”‚  (OHLCV + L2)    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                 â–¼                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚Model Trainingâ”‚  â”‚ Backtesting  â”‚  â”‚Live Trading  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1. Data Collection Phase

### Step 1: Configure L2 Collection
```yaml
# config_l2.yaml
l2_collection:
  enabled: true
  websocket_depth: 50  # Number of price levels
  collection_duration_seconds: 86400  # 24 hours
  max_file_size_mb: 50
  compression: gzip
  
# Symbols to collect (can differ from trading symbol)
l2_symbols:
  - BTC/USDT:USDT
  - ETH/USDT:USDT
```

### Step 2: Run L2 Collector
```python
# run_l2_collector.py
import yaml
from l2_data_collector import L2DataCollector

# Load config
with open('config_l2.yaml', 'r') as f:
    config = yaml.safe_load(f)

# Start collection
collector = L2DataCollector(config, './')
collector.start_collection_websocket()
```

## 2. Data Storage Structure

### L2 Data Format (Compressed JSONL)
```json
{
  "exchange": "bybit",
  "symbol": "BTCUSDT",
  "timestamp_ms": 1716825600000,
  "received_timestamp_ms": 1716825600123,
  "type": "snapshot",
  "bids": [[69500.0, 1.234], [69499.5, 2.456], ...],
  "asks": [[69501.0, 0.987], [69501.5, 1.543], ...],
  "update_id": 123456789,
  "sequence_id": 987654321
}
```

### Directory Structure
```
l2_data/
â”œâ”€â”€ l2_data_BTCUSDT_20240525_120000.jsonl.gz
â”œâ”€â”€ l2_data_BTCUSDT_20240525_180000.jsonl.gz
â”œâ”€â”€ l2_data_BTCUSDT_20240526_000000.jsonl.gz
â””â”€â”€ ...
```

## 3. Data Alignment Strategy

### Challenge: Aligning L2 with OHLCV
- OHLCV: 1-minute candles (fixed intervals)
- L2: Continuous stream (irregular intervals)

### Solution: Time-based Alignment
```python
def align_l2_with_ohlcv(ohlcv_df, l2_data_path, timeframe='1m'):
    """
    Aligns L2 data with OHLCV candles
    
    Strategy:
    1. For each OHLCV candle, find the L2 snapshot closest to candle close time
    2. Calculate L2 features from that snapshot
    3. Merge features with OHLCV data
    """
    # Implementation in next section
```

## 4. Feature Engineering Integration

### L2 Features to Extract
1. **Order Book Imbalance**
   - `bid_volume / (bid_volume + ask_volume)` at various depths
   
2. **Spread Metrics**
   - Bid-ask spread (absolute and percentage)
   - Weighted mid-price
   
3. **Depth Metrics**
   - Total bid/ask volume at different levels
   - Volume-weighted average price (VWAP) from order book
   
4. **Microstructure Features**
   - Order book slope/curvature
   - Price impact estimates
   - Queue position indicators

### Implementation in FeatureEngineer
```python
def calculate_l2_features_aligned(self, ohlcv_df, l2_data_dir):
    """
    Calculate L2 features aligned with OHLCV data
    """
    # Load L2 data for the time range
    l2_snapshots = self.load_l2_data_for_timerange(
        l2_data_dir, 
        ohlcv_df['timestamp'].min(),
        ohlcv_df['timestamp'].max()
    )
    
    # Align and calculate features
    l2_features = []
    for idx, row in ohlcv_df.iterrows():
        # Find nearest L2 snapshot
        snapshot = self.find_nearest_l2_snapshot(
            l2_snapshots, 
            row['timestamp']
        )
        
        # Calculate features
        features = self.calculate_l2_features_from_snapshot(snapshot)
        l2_features.append(features)
    
    return pd.DataFrame(l2_features, index=ohlcv_df.index)
```

## 5. User Flow for L2 Integration

### Flow 1: Historical Model Training with L2
```
1. Collect L2 data (run continuously in background)
   â””â”€> python run_l2_collector.py

2. Download/prepare OHLCV data
   â””â”€> python download_sample_data.py

3. Align and merge L2 with OHLCV
   â””â”€> python align_l2_data.py

4. Generate features (including L2)
   â””â”€> python run_trading_bot.py (option 3: data prep)

5. Train model with L2 features
   â””â”€> python run_trading_bot.py (option 1: standard)
```

### Flow 2: Backtesting with L2
```
1. Ensure L2 data covers backtest period
2. Run backtest with L2-enabled config
   â””â”€> python run_backtest_l2.py
```

### Flow 3: Live Trading with L2
```
1. Start L2 collector (if not running)
2. Configure real-time L2 feature calculation
3. Run live trading with L2 features
   â””â”€> python run_live_trading_l2.py
```

## 6. Implementation Files Needed

### 1. `align_l2_data.py`
```python
"""
Aligns L2 data with OHLCV data for training
"""
import pandas as pd
import gzip
import json
from datetime import datetime, timedelta

class L2DataAligner:
    def __init__(self, config):
        self.config = config
        self.l2_data_dir = config.get('l2_data_folder', 'l2_data')
        
    def load_l2_snapshots(self, start_time, end_time):
        """Load L2 snapshots for given time range"""
        # Implementation here
        
    def align_with_ohlcv(self, ohlcv_df):
        """Align L2 data with OHLCV candles"""
        # Implementation here
```

### 2. `run_backtest_l2.py`
```python
"""
Run backtest with L2 data integration
"""
# Modified version of run_backtest.py that includes L2 alignment
```

### 3. `run_live_trading_l2.py`
```python
"""
Live trading with real-time L2 features
"""
# Real-time L2 feature calculation
```

## 7. Configuration Updates

### Update `config.yaml` for L2
```yaml
# L2 Data Settings
use_l2_features: true
use_l2_features_for_training: true
l2_data_folder: l2_data
l2_alignment_method: nearest  # or 'interpolate'
l2_max_time_diff_ms: 1000  # Max time difference for alignment

# L2 Feature Configuration
l2_features:
  - bid_ask_spread
  - bid_ask_spread_pct
  - order_book_imbalance_5
  - order_book_imbalance_10
  - order_book_imbalance_20
  - total_bid_volume_10
  - total_ask_volume_10
  - weighted_mid_price
  - price_impact_buy
  - price_impact_sell

l2_depth_levels: [5, 10, 20, 50]
```

## 8. Challenges and Solutions

### Challenge 1: Data Volume
**Problem**: L2 data is massive (GBs per day)
**Solution**: 
- Compress data (gzip)
- Store only necessary depth levels
- Implement data retention policy

### Challenge 2: Time Alignment
**Problem**: L2 updates don't align with OHLCV candles
**Solution**:
- Use nearest snapshot method
- Interpolate between snapshots
- Cache aligned data for reuse

### Challenge 3: Missing Data
**Problem**: L2 collector might have gaps
**Solution**:
- Implement data quality checks
- Use forward-fill for small gaps
- Mark features as NaN for large gaps

### Challenge 4: Real-time Processing
**Problem**: Calculating L2 features in real-time
**Solution**:
- Maintain in-memory L2 state
- Update features incrementally
- Use efficient data structures

## 9. Testing Strategy

### Unit Tests
```python
def test_l2_alignment():
    """Test L2 data alignment with OHLCV"""
    # Test implementation
    
def test_l2_feature_calculation():
    """Test L2 feature calculation accuracy"""
    # Test implementation
```

### Integration Tests
- Test full pipeline with sample L2 data
- Verify feature consistency
- Check performance metrics

## 10. Performance Considerations

### Memory Management
- Stream L2 data instead of loading all at once
- Use chunking for large datasets
- Implement garbage collection

### Processing Speed
- Parallelize L2 feature calculation
- Use vectorized operations
- Cache frequently accessed data

### Storage Optimization
- Rotate old L2 files
- Implement archival strategy
- Use efficient compression

## Conclusion

Integrating L2 data requires careful planning for:
1. **Collection**: Continuous WebSocket streaming
2. **Storage**: Compressed, time-indexed format
3. **Alignment**: Matching L2 snapshots with OHLCV candles
4. **Feature Engineering**: Extracting meaningful signals
5. **Integration**: Seamless use in training/backtesting/live

The key is maintaining data quality and ensuring consistent feature calculation across all phases of the trading system. 

# L2 Order Book Integration Guide

## Overview

This guide covers the complete Level 2 order book integration for the cryptocurrency trading bot, including the recent optimization based on actual order book depth analysis.

## ðŸŽ¯ Recent Optimization (Current Session)

### Order Book Depth Analysis Results
**Analyzed 1,084,954 L2 snapshots from Bybit BTC/USDT:**
- **Median depth**: 1 level for both bids and asks
- **5+ levels**: Only 14.1% of snapshots
- **10+ levels**: Only 5.7% of snapshots  
- **20+ levels**: Only 2.6% of snapshots

### Configuration Optimization
**Before optimization:**
```yaml
l2_depth_imbalance_levels: [5, 10, 20]  # Unrealistic
l2_price_impact_depth_idx: 4            # 5th level (rarely available)
l2_curve_fit_levels: 20                 # Too deep
l2_max_time_diff_ms: 1000               # Too strict
```

**After optimization:**
```yaml
l2_depth_imbalance_levels: [2, 3]       # Realistic depths
l2_price_impact_depth_idx: 1            # 2nd level (more available)
l2_curve_fit_levels: 3                  # Realistic for slope analysis
l2_max_time_diff_ms: 30000              # 30 seconds tolerance
```

### Performance Improvements
- **NaN reduction**: 2,191 â†’ 1,713 (22% improvement)
- **Core feature coverage**: 0% â†’ 100%
- **Data quality**: 41.8% of snapshots pass quality filters
- **Alignment success**: 100% coverage with window-based matching

## ðŸ“Š L2 Feature Categories

### Core Features (100% Coverage)
These features are available for virtually all candles:

```python
# Spread metrics
'bid_ask_spread'         # Absolute spread (best_ask - best_bid)
'bid_ask_spread_pct'     # Percentage spread
'weighted_mid_price'     # (best_bid + best_ask) / 2

# Order book imbalance (optimized depths)
'order_book_imbalance_2' # Bid/ask volume ratio at 2 levels
'order_book_imbalance_3' # Bid/ask volume ratio at 3 levels

# Volume metrics
'total_bid_volume_2'     # Total bid volume at 2 levels
'total_ask_volume_2'     # Total ask volume at 2 levels
'total_bid_volume_3'     # Total bid volume at 3 levels
'total_ask_volume_3'     # Total ask volume at 3 levels
```

### Advanced Features (Partial Coverage)
These features require deeper order book data:

```python
# Price impact analysis (~45% coverage)
'price_impact_buy'       # Impact of buying 1 BTC
'price_impact_sell'      # Impact of selling 1 BTC

# Order book slope (~20% coverage)
'bid_slope'             # Linear regression slope of bid side
'ask_slope'             # Linear regression slope of ask side
```

## ðŸ”§ Implementation Details

### 1. Data Collection (`l2_data_collector.py`)

**WebSocket Configuration:**
```python
# Optimized for Bybit
websocket_url = "wss://stream.bybit.com/v5/public/linear"
subscription = "orderbook.50.BTCUSDT"  # 50 levels depth
```

**Data Quality Filtering:**
```python
def is_valid_snapshot(bids, asks):
    """Filter out invalid snapshots"""
    if not bids or not asks:
        return False
    
    best_bid = float(bids[0][0])
    best_ask = float(asks[0][0])
    
    # Filter negative spreads and invalid prices
    return (best_bid > 0 and best_ask > 0 and 
            best_ask >= best_bid)
```

### 2. Data Alignment (`align_l2_data.py`)

**Window-Based Alignment:**
```python
def align_l2_with_ohlcv(self, ohlcv_df):
    """Use window-based matching for better coverage"""
    for candle_time in ohlcv_df['timestamp']:
        candle_start = candle_time
        candle_end = candle_time + pd.Timedelta(minutes=1)
        
        # Find snapshots within 1-minute window
        window_snapshots = l2_df[
            (l2_df['timestamp_ms'] >= candle_start_ms) & 
            (l2_df['timestamp_ms'] < candle_end_ms)
        ]
        
        if not window_snapshots.empty:
            # Use closest to candle close
            closest_snapshot = find_closest_to_target(window_snapshots, candle_time)
        else:
            # Fallback to nearest within tolerance (30s)
            closest_snapshot = find_nearest_within_tolerance(l2_df, candle_time)
```

### 3. Feature Calculation

**Optimized Depth Usage:**
```python
def calculate_l2_features(self, snapshot):
    """Calculate L2 features with realistic depth expectations"""
    bids = snapshot['bids']
    asks = snapshot['asks']
    
    # Use configured depth levels
    depth_levels = self.config.get('l2_depth_imbalance_levels', [2, 3])
    
    for depth in depth_levels:
        # Only calculate if sufficient depth available
        if len(bids) >= depth and len(asks) >= depth:
            bid_volume = sum(float(level[1]) for level in bids[:depth])
            ask_volume = sum(float(level[1]) for level in asks[:depth])
            total_volume = bid_volume + ask_volume
            
            features[f'order_book_imbalance_{depth}'] = (
                bid_volume / total_volume if total_volume > 0 else 0.5
            )
        else:
            features[f'order_book_imbalance_{depth}'] = np.nan
```

## ðŸ“ˆ Configuration Guide

### Recommended Settings

**For High-Frequency Trading:**
```yaml
l2_depth_imbalance_levels: [2]           # Focus on top levels
l2_price_impact_depth_idx: 0             # Best bid/ask only
l2_curve_fit_levels: 2                   # Minimal slope analysis
l2_max_time_diff_ms: 5000                # 5 seconds tolerance
```

**For Medium-Frequency Trading (Recommended):**
```yaml
l2_depth_imbalance_levels: [2, 3]        # 2-3 levels balance
l2_price_impact_depth_idx: 1             # 2nd level impact
l2_curve_fit_levels: 3                   # 3-level slope
l2_max_time_diff_ms: 30000               # 30 seconds tolerance
```

**For Research/Analysis:**
```yaml
l2_depth_imbalance_levels: [2, 3, 5]     # Include 5 levels
l2_price_impact_depth_idx: 2             # 3rd level impact
l2_curve_fit_levels: 5                   # Deeper analysis
l2_max_time_diff_ms: 60000               # 1 minute tolerance
```

## ðŸ” Quality Monitoring

### 1. Order Book Depth Analysis
```bash
# Analyze your L2 data depth distribution
python analyze_l2_collection.py
```

**Expected Output:**
```
ðŸ“Š BID LEVELS ANALYSIS
Mean bid levels: 2.9
Median bid levels: 1
Snapshots with â‰¥5 bid levels: 152661 (14.1%)
Snapshots with â‰¥10 bid levels: 61380 (5.7%)
Snapshots with â‰¥20 bid levels: 27968 (2.6%)

ðŸ’¡ RECOMMENDATIONS
âš ï¸  Less than 50% of snapshots have â‰¥20 levels
   Consider using 5 or 10 levels for L2 features
âœ… 5-level features should be very reliable
```

### 2. L2 Data Quality Check
```bash
# Check feature coverage and NaN counts
python check_l2_quality.py
```

**Expected Output:**
```
L2 Data Quality Analysis
========================================
Total rows: 362
Total columns: 14

NaN counts by feature:
  price_impact_buy: 197 (54.4%)
  price_impact_sell: 212 (58.6%)
  bid_slope: 290 (80.1%)
  ask_slope: 290 (80.1%)

Key feature availability:
  bid_ask_spread: 362/362 (100%)
  order_book_imbalance_2: 362/362 (100%)
  order_book_imbalance_3: 362/362 (100%)
```

### 3. Alignment Diagnostics
```bash
# Analyze timestamp alignment quality
python l2_alignment_diagnostics.py
```

## ðŸš¨ Common Issues & Solutions

### Issue 1: High NaN Values
**Problem**: Many L2 features showing NaN values
**Cause**: Using unrealistic depth levels (5, 10, 20)
**Solution**: Use optimized configuration with depths [2, 3]

```yaml
# Fix configuration
l2_depth_imbalance_levels: [2, 3]  # Instead of [5, 10, 20]
```

### Issue 2: Low Feature Coverage
**Problem**: Advanced features (slope, impact) have low coverage
**Cause**: Order book depth limitations
**Solution**: Focus on core features, accept partial coverage for advanced features

```python
# In model training, handle NaN appropriately
# Core features: 100% coverage - use directly
# Advanced features: ~20-45% coverage - use with caution
```

### Issue 3: Alignment Failures
**Problem**: L2 data not aligning with OHLCV
**Cause**: Strict timestamp matching
**Solution**: Use window-based alignment with tolerance

```yaml
# Increase tolerance
l2_max_time_diff_ms: 30000  # 30 seconds instead of 1 second
```

### Issue 4: Data Quality Issues
**Problem**: Invalid spreads or missing data
**Cause**: Market conditions or exchange issues
**Solution**: Implement quality filtering

```python
# Quality filters implemented:
# - Positive prices only
# - Valid spreads (ask >= bid)
# - Non-empty order book sides
# Result: ~41.8% of snapshots pass filters
```

## ðŸ“Š Performance Expectations

### Data Quality Metrics
- **Raw snapshots**: 1,084,954 collected
- **Valid snapshots**: 41.8% pass quality filters
- **Core feature coverage**: 100% with optimized config
- **Advanced feature coverage**: 19.9-45.6%

### Model Performance Impact
- **Feature count**: 13 L2 features (out of 37 total)
- **Training improvement**: L2 features contribute to model signal
- **NaN handling**: Automatic forward-fill and zero-fill
- **Prediction success**: 93.4% of candles get predictions

### Storage Requirements
- **Compressed size**: ~5.4MB per hour
- **6-hour collection**: ~32MB total
- **Format**: JSONL with gzip compression

## ðŸ”¬ Advanced Usage

### Custom Feature Development
```python
def calculate_custom_l2_feature(self, snapshot):
    """Add custom L2 analysis"""
    bids = snapshot['bids']
    asks = snapshot['asks']
    
    # Use realistic depth expectations
    if len(bids) >= 2 and len(asks) >= 2:
        # Your custom analysis here
        # Example: Volume-weighted average price
        bid_vwap = sum(float(p)*float(v) for p,v in bids[:2]) / sum(float(v) for p,v in bids[:2])
        ask_vwap = sum(float(p)*float(v) for p,v in asks[:2]) / sum(float(v) for p,v in asks[:2])
        return {'custom_vwap_spread': ask_vwap - bid_vwap}
    
    return {'custom_vwap_spread': np.nan}
```

### Real-Time Feature Calculation
```python
def calculate_realtime_l2_features(self, live_snapshot):
    """Calculate L2 features for live trading"""
    # Use same optimized configuration
    features = self.calculate_l2_features(live_snapshot)
    
    # Handle NaN values for live trading
    for key, value in features.items():
        if pd.isna(value):
            features[key] = 0.0  # Or use last known value
    
    return features
```

## ðŸŽ¯ Best Practices

### 1. Configuration
- Start with recommended medium-frequency settings
- Analyze your specific data with `analyze_l2_collection.py`
- Adjust depth levels based on actual availability

### 2. Feature Selection
- Always include core features (spread, imbalance at 2-3 levels)
- Use advanced features (slope, impact) with awareness of coverage limitations
- Monitor feature importance in your models

### 3. Quality Monitoring
- Regularly check L2 data quality
- Monitor alignment success rates
- Track feature coverage over time

### 4. Model Training
- Handle NaN values appropriately (forward-fill, zero-fill)
- Consider feature importance when selecting L2 features
- Test model performance with and without L2 features

## ðŸ“š References

### Key Files
- `l2_data_collector.py`: WebSocket data collection
- `align_l2_data.py`: L2-OHLCV alignment with optimization
- `analyze_l2_collection.py`: Order book depth analysis
- `featureengineer.py`: L2 feature integration
- `config.yaml`: Optimized L2 configuration

### Analysis Scripts
- `check_l2_quality.py`: Feature quality analysis
- `l2_diagnostics.py`: Comprehensive L2 diagnostics
- `test_l2_integration.py`: Integration testing

---

**Last Updated**: Current session - Order book depth optimization complete
**Status**: âœ… Production ready with realistic depth expectations 